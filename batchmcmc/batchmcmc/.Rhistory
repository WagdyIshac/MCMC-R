#Package installation if missing
install.packages("Rcpp")
install.packages("hitandrun")
install.packages("devtools")
devtools::install_github("Azure/rAzureBatch")#, force = TRUE)
devtools::install_github("Azure/doAzureParallel", force = TRUE)
#update.packages("bitops")
setwd("c:/code/mcmc/batchmcmc/batchmcmc")
install.packages("bitops")
install.packages("PerformanceAnalytics")
# Load the doAzureParallel library
library(doAzureParallel)
library("Rcpp")
library("hitandrun")
source("hpcutilTS.R")
library(devtools)
#setting Azure credentials
setCredentials("credentials55.json")
if (length(getClusterList()[, 1]) == 0) {
clusterHPC77 <- makeCluster(cluster = "clusterHPC.json")
} else {
clusterHPC77 <- getCluster("mcmchpc777") #if the cluster already exists this command retrieve the cluster
#register the parallel methods on the cluster and check the parallel workers
}
registerDoAzureParallel(clusterHPC77)
getDoParWorkers()
outputFoldername <- "ivestec_demo"
returnURL <- harSetAzureStorage(outputFoldername)
start_p <- Sys.time()
path = "Test27Feb2019/"
out.file <- ""
file1 <- dir(path, pattern = "file1{1}",full.names = FALSE)
file2 <- dir(path, pattern = "file2{1}", full.names = FALSE)
file3 <- dir(path, pattern = "file3{1}", full.names = FALSE)
idFiles <- dir(path, pattern = "idFile{1}", full.names = FALSE)
for (i in 1:length(file1)) {
file <- as.matrix(read.csv(file = paste0(path, file1[i]), header = FALSE, sep = ","))
if (file[1, 1] == 0)
{
outputFoldername <- paste0(gsub(".csv","", gsub("_","",file1[i])), "-20211115")
#returnURL <- harSetAzureStorage(outputFoldername)
harfileexec2(paste0(path, file1[i]), NULL, paste0(path, file3[i]), paste0(path, idFiles[i]), 0, outputFoldername, fileoutputurl = returnURL)
} else {
outputFoldername <- paste0(gsub(".csv", "", gsub("_", "", file1[i])), "-20211115")
#returnURL <- harSetAzureStorage(outputFoldername)
harfileexec2(paste0(path, file1[i]), paste0(path, file2[i]), paste0(path, file3[i]), paste0(path, idFiles[i]), 1, outputFoldername, fileoutputurl = returnURL )
}
}
outputFoldername <- "ivestecdemo"
returnURL <- harSetAzureStorage(outputFoldername)
start_p <- Sys.time()
for (i in 1:length(file1)) {
file <- as.matrix(read.csv(file = paste0(path, file1[i]), header = FALSE, sep = ","))
if (file[1, 1] == 0)
{
outputFoldername <- paste0(gsub(".csv","", gsub("_","",file1[i])), "-20211115")
#returnURL <- harSetAzureStorage(outputFoldername)
harfileexec2(paste0(path, file1[i]), NULL, paste0(path, file3[i]), paste0(path, idFiles[i]), 0, outputFoldername, fileoutputurl = returnURL)
} else {
outputFoldername <- paste0(gsub(".csv", "", gsub("_", "", file1[i])), "-20211115")
#returnURL <- harSetAzureStorage(outputFoldername)
harfileexec2(paste0(path, file1[i]), paste0(path, file2[i]), paste0(path, file3[i]), paste0(path, idFiles[i]), 1, outputFoldername, fileoutputurl = returnURL )
}
}
stopCluster(clusterHPC77)
if (length(getClusterList()[, 1]) == 0) {
clusterHPC77 <- makeCluster(cluster = "clusterHPC.json")
} else {
clusterHPC77 <- getCluster("mcmchpc777") #if the cluster already exists this command retrieve the cluster
#register the parallel methods on the cluster and check the parallel workers
}
registerDoAzureParallel(clusterHPC77)
getDoParWorkers()
outputFoldername <- "ivesteccalldemo"
returnURL <- harSetAzureStorage(outputFoldername)
start_p <- Sys.time()
path = "Test27Feb2019/"
out.file <- ""
file1 <- dir(path, pattern = "file1{1}",full.names = FALSE)
file2 <- dir(path, pattern = "file2{1}", full.names = FALSE)
file3 <- dir(path, pattern = "file3{1}", full.names = FALSE)
idFiles <- dir(path, pattern = "idFile{1}", full.names = FALSE)
for (i in 1:length(file1)) {
file <- as.matrix(read.csv(file = paste0(path, file1[i]), header = FALSE, sep = ","))
if (file[1, 1] == 0)
{
outputFoldername <- paste0(gsub(".csv","", gsub("_","",file1[i])), "-20211115")
#returnURL <- harSetAzureStorage(outputFoldername)
harfileexec2(paste0(path, file1[i]), NULL, paste0(path, file3[i]), paste0(path, idFiles[i]), 0, outputFoldername, fileoutputurl = returnURL)
} else {
outputFoldername <- paste0(gsub(".csv", "", gsub("_", "", file1[i])), "-20211115")
#returnURL <- harSetAzureStorage(outputFoldername)
harfileexec2(paste0(path, file1[i]), paste0(path, file2[i]), paste0(path, file3[i]), paste0(path, idFiles[i]), 1, outputFoldername, fileoutputurl = returnURL )
}
}
stopCluster(clusterHPC77)
